{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a46f4-568b-443e-bb2a-c221182c2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join, isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a09e93-dc2c-4754-9a4d-ec52974adada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0e06c-a2af-45cc-b58a-df1950551094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "print([f.name for f in fm.fontManager.ttflist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba501d5f-bc94-4f84-a280-323013d208af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = 'DejaVu Serif'\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "plt.rcParams[\"mathtext.fontset\"] = 'dejavuserif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652a064-6990-4471-8e08-4c6eeab98e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename, suffix):\n",
    "    # ex: Qwen2.5-3B_codecontests_revision=main_temp=1.0_all_ids.pkl\n",
    "    filename = filename[:filename.index(suffix)] # Qwen2.5-3B_codecontests_revision=main_temp=1.0\n",
    "    i = filename.index('_')\n",
    "    model_name, filename = filename[:i], filename[i+1:] # codecontests_revision=main_temp=1.0\n",
    "    i = filename.index('_')\n",
    "    dataset_name, filename = filename[:i], filename[i+1:] # revision=main_temp=1.0\n",
    "    i = filename.index('_')\n",
    "    revision, temperature = filename[:i], filename[i+1:]\n",
    "    temperature = temperature[temperature.index('=')+1:]\n",
    "    revision = revision[revision.index('=')+1:]\n",
    "    return model_name, dataset_name, revision, float(temperature) # Qwen2.5-3B, codecontests, main, 1.0\n",
    "\n",
    "def is_chat_model(model_name, instruct=True):\n",
    "    if instruct:\n",
    "        return 'chat' in model_name.lower() or 'instruct' in model_name.lower()\n",
    "    else:\n",
    "        return not is_chat_model(model_name, instruct=True)\n",
    "\n",
    "def is_model_family(model_name, model_family):\n",
    "    return model_family.lower() in model_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3f0a5-d614-44a1-b05f-1e960d70a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(folder, model_family, dataset_name, instruct, suffix):\n",
    "    all_files = {}\n",
    "    for filename in listdir(folder):\n",
    "        full_path = join(folder, filename)\n",
    "        if isfile(full_path) and full_path.endswith(suffix):\n",
    "            load_model_name, load_dataset_name, load_revision, load_temperature = parse_filename(filename, suffix)\n",
    "\n",
    "            if is_model_family(load_model_name, model_family) and dataset_name == load_dataset_name and is_chat_model(load_model_name, instruct):\n",
    "                load_model_revision_name = '{}_revision={}'.format(load_model_name, load_revision)\n",
    "                with open(full_path, 'rb') as f:\n",
    "                    saved_file = pickle.load(f)\n",
    "                if load_model_revision_name not in all_files:\n",
    "                    all_files[load_model_revision_name] = {}\n",
    "                all_files[load_model_revision_name][load_temperature] = saved_file\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f4f1d-54a4-425b-94fd-2a7988f091ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model_revision_name):\n",
    "    # Qwen2.5-3B_revision=main\n",
    "    model_name = model_revision_name[:model_revision_name.index('_')] # Qwen2.5-3B\n",
    "    while '-' in model_name:\n",
    "        prefix, model_name = model_name[:model_name.index('-')], model_name[model_name.index('-')+1:]\n",
    "        if prefix.lower().endswith('b'):\n",
    "            return float(prefix[:-1])\n",
    "        elif prefix.lower().endswith('m'):\n",
    "            return float(prefix[:-1]) / 1000\n",
    "    if model_name.lower().endswith('b'):\n",
    "        return float(model_name[:-1])\n",
    "    elif model_name.lower().endswith('m'):\n",
    "        return float(model_name[:-1]) / 1000\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "def compute_time_avg(inp):\n",
    "    # inp: List[List[float]]\n",
    "    max_len = np.max([len(lst) for lst in inp])\n",
    "    totals = np.zeros((max_len,))\n",
    "    counts = np.zeros((max_len,))\n",
    "    for lst in inp:\n",
    "        totals[:len(lst)] += lst\n",
    "        counts[:len(lst)] += 1\n",
    "    avg = totals / counts\n",
    "    overall_avg = np.sum(totals) / np.sum(counts)\n",
    "    \n",
    "    totals_std = np.zeros((max_len,))\n",
    "    counts_std = np.zeros((max_len,))\n",
    "    for lst in inp:\n",
    "        totals_std[:len(lst)] += (np.array(lst) - avg[:len(lst)])**2\n",
    "        counts_std[:len(lst)] += 1\n",
    "    std = np.sqrt(totals_std / (counts_std**2)) # sample mean variance = population variance / count\n",
    "    return avg, overall_avg, std\n",
    "\n",
    "def load_ent_over_time(folder, model_family, dataset_name, max_len):\n",
    "    suffix = '_logprobs_and_ents.pkl'\n",
    "    all_logprobs_and_ents = load_files(folder, model_family, dataset_name, False, suffix)\n",
    "    plot_data = []\n",
    "    for model_revision_name in all_logprobs_and_ents.keys():\n",
    "        if 1.0 in all_logprobs_and_ents[model_revision_name].keys():\n",
    "            response_logprobs, generation_ents = all_logprobs_and_ents[model_revision_name][1.0]\n",
    "            response_logprobs = [lp[:max_len] for lp in response_logprobs]\n",
    "            generation_ents = [ents[:max_len] for ents in generation_ents]\n",
    "            ents_over_time, ents_avg, ents_std = compute_time_avg(generation_ents)\n",
    "            logprobs_over_time, logprobs_avg, logprobs_std = compute_time_avg(response_logprobs)\n",
    "\n",
    "            data = {}\n",
    "            data['ents'] = ents_over_time\n",
    "            data['logprobs'] = -1 * logprobs_over_time\n",
    "            data['size'] = np.log(get_model_size(model_revision_name))\n",
    "            data['name'] = model_revision_name[:model_revision_name.index('_')]\n",
    "            plot_data.append(data)\n",
    "    return plot_data\n",
    "\n",
    "def exponential_smoothing(arr, alpha):\n",
    "    \"\"\"\n",
    "    Applies exponential smoothing to a time series.\n",
    "\n",
    "    Parameters:\n",
    "        arr (array-like): The input array containing floats.\n",
    "        alpha (float): The smoothing factor, where 0 < alpha <= 1.\n",
    "                      - Higher alpha gives more weight to recent values.\n",
    "                      - Lower alpha gives more weight to older values.\n",
    "\n",
    "    Returns:\n",
    "        list: The smoothed time series.\n",
    "    \"\"\"\n",
    "    if not (0 < alpha <= 1):\n",
    "        raise ValueError(\"Alpha must be between 0 and 1.\")\n",
    "    if not isinstance(arr, (list, np.ndarray)):\n",
    "        raise TypeError(\"Input must be a list or numpy array.\")\n",
    "    if len(arr) == 0:\n",
    "        raise ValueError(\"Input array must not be empty.\")\n",
    "    \n",
    "    smoothed = [arr[0]]  # Initialize with the first value of the series\n",
    "    for i in range(1, len(arr)):\n",
    "        smoothed_value = alpha * arr[i] + (1 - alpha) * smoothed[-1]\n",
    "        smoothed.append(smoothed_value)\n",
    "    \n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d640d04-cf03-443b-99f6-6277257e139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_cmap(cmap, start_frac=0., end_frac=0.9):\n",
    "    N = 256  # Number of colors in the original colormap\n",
    "    start = int(N * start_frac)  # Start index for truncation\n",
    "    end = int(N * end_frac)  # End index for truncation\n",
    "    colors = cmap(np.linspace(start / N, end / N, end - start))\n",
    "    truncated_cmap = mcolors.ListedColormap(colors)\n",
    "    return truncated_cmap\n",
    "\n",
    "def plot_ent_over_time(plot_data, dataset_name, alpha1=0.2, alpha2=0.1, ax=None, show_xlabel=True, show_title=True):\n",
    "    # Normalize sizes for colormap\n",
    "    sizes = [d['size'] for d in plot_data]\n",
    "    norm = plt.Normalize(min(sizes), max(sizes))\n",
    "    cmap = plt.cm.plasma\n",
    "    cmap = truncate_cmap(cmap)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        do_show = True\n",
    "    else:\n",
    "        do_show = False\n",
    "    \n",
    "    # Plot each dataset\n",
    "    legend_items = []\n",
    "    \n",
    "    for data in plot_data:\n",
    "        color = cmap(norm(data['size']))\n",
    "        ax.plot(exponential_smoothing(data['ents'], alpha1), linestyle='-', color=color, linewidth=3, zorder=1)  # Solid line for 'ents'\n",
    "        ax.plot(exponential_smoothing(data['logprobs'], alpha2), linestyle='--', color=color, linewidth=1, zorder=0)  # Dashed line for 'logprobs'\n",
    "        # Collect legend items (size, name, color)\n",
    "        legend_items.append((data['size'], data['name'], color))\n",
    "    \n",
    "    # Sort legend items by size\n",
    "    legend_items = sorted(legend_items, key=lambda x: x[0])\n",
    "    \n",
    "    # Create legend\n",
    "    legend_patches = [plt.Line2D([0], [0], color=item[2], lw=2, label=item[1]) for item in legend_items]\n",
    "    size_legend = ax.legend(handles=legend_patches, loc='upper left')#, title=\"Legend (Sorted by Size)\")\n",
    "\n",
    "    line_legend_patches = [\n",
    "        plt.Line2D([0], [0], linestyle='-', color='grey', lw=2, label='Entropy'),\n",
    "        plt.Line2D([0], [0], linestyle='--', color='grey', lw=1, label='Log Loss')\n",
    "    ]\n",
    "    line_legend = ax.legend(handles=line_legend_patches, loc='upper right')#, title=\"Line Type\"\n",
    "\n",
    "    ax.add_artist(size_legend)\n",
    "    ax.add_artist(line_legend)\n",
    "    \n",
    "    # Labels\n",
    "    if show_xlabel:\n",
    "        ax.set_xlabel(\"Step\", fontsize=18, weight='bold')\n",
    "    #ax.set_ylabel(\"Values\")\n",
    "    if show_title:\n",
    "        ax.set_title(dataset_name, fontsize=20)\n",
    "\n",
    "    if do_show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_ent_over_time_multiple(all_plot_datas, dataset_names, save_path, alpha1=0.2, alpha2=0.1):\n",
    "    fig, all_axs = plt.subplots(4, 3, figsize=(16, 16))\n",
    "    for i, (axs, plot_datas) in enumerate(zip(all_axs, all_plot_datas)):\n",
    "        for j, (ax, plot_data, dataset_name) in enumerate(zip(axs, plot_datas, dataset_names)):\n",
    "            plot_ent_over_time(plot_data, dataset_names_mapping[dataset_name], ax=ax, alpha1=alpha1, alpha2=alpha2, \n",
    "                               show_title=(i==0), show_xlabel=(i==3 and j==1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1dd64-28e3-4abf-aeeb-124824df61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'results'\n",
    "model_families = ('Llama-3', 'Qwen2.5', 'pythia', 'Llama-2')\n",
    "dataset_names = ('wikitext', 'writingprompts', 'codecontests')\n",
    "dataset_names_mapping = {\n",
    "    'wikitext' : 'WikiText', \n",
    "    'writingprompts': 'WritingPrompts',\n",
    "    'codecontests': 'CodeContests'\n",
    "}\n",
    "model_family_mapping = {\n",
    "    'Llama-3': 'Llama 3', \n",
    "    'Qwen2.5': 'Qwen2.5', \n",
    "    'pythia': 'Pythia', \n",
    "    'Llama-2': 'Llama 2'\n",
    "}\n",
    "max_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41719e1d-8a38-46e6-a381-5d51a516f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_datas = []\n",
    "for model_family in model_families:\n",
    "    plot_datas = []\n",
    "    for dataset_name in dataset_names:\n",
    "        plot_data = load_ent_over_time(save_folder, model_family, dataset_name, max_len)\n",
    "        plot_datas.append(plot_data)\n",
    "    all_plot_datas.append(plot_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca607d7-0d3d-4f6e-9626-6c355f72d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}/ent_over_time_plot_nosmoothing.png'.format(save_folder)\n",
    "plot_ent_over_time_multiple(\n",
    "    [all_plot_datas[3], all_plot_datas[0], all_plot_datas[2], all_plot_datas[1]], dataset_names, save_path, alpha1=1, alpha2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29342ca-17d3-48a8-a839-afc5a5507119",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}/ent_over_time_plot.png'.format(save_folder)\n",
    "plot_ent_over_time_multiple([all_plot_datas[3], all_plot_datas[0], all_plot_datas[2], all_plot_datas[1]], dataset_names, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36d1ec-1d9b-4544-a676-df423ac3fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ent_vs_size(folder, model_family, dataset_name, max_len):\n",
    "    suffix = '_logprobs_and_ents.pkl'\n",
    "    all_logprobs_and_ents = load_files(folder, model_family, dataset_name, False, suffix)\n",
    "    plot_data = []\n",
    "    for model_revision_name in all_logprobs_and_ents.keys():\n",
    "        if 1.0 in all_logprobs_and_ents[model_revision_name].keys():\n",
    "            response_logprobs, generation_ents = all_logprobs_and_ents[model_revision_name][1.0]\n",
    "            response_logprobs = [lp[:max_len] for lp in response_logprobs]\n",
    "            generation_ents = [ents[:max_len] for ents in generation_ents]\n",
    "            ents_over_time, ents_avg, ents_std = compute_time_avg(generation_ents)\n",
    "            logprobs_over_time, logprobs_avg, logprobs_std = compute_time_avg(response_logprobs)\n",
    "\n",
    "            data = {}\n",
    "            data['entCE'] = np.log(ents_avg - (-1 * logprobs_avg))\n",
    "            data['size'] = np.log(get_model_size(model_revision_name))\n",
    "            #data['name'] = model_revision_name[:model_revision_name.index('_')]\n",
    "            plot_data.append(data)\n",
    "    return plot_data\n",
    "\n",
    "def plot_ent_vs_size(plot_datas, dataset_names, model_family, ax=None, show_ylabel=True):\n",
    "    \"\"\"\n",
    "    Plots scatter plots for each plot_data in plot_datas and adds best fit lines.\n",
    "    \n",
    "    Parameters:\n",
    "        plot_datas (list of list of dict): Each plot_data is a list of dicts with 'size' (x-axis) and 'entCE' (y-axis).\n",
    "        dataset_names (list of str): Names of the datasets, corresponding to each plot_data in plot_datas.\n",
    "    \"\"\"\n",
    "    if len(plot_datas) != len(dataset_names):\n",
    "        raise ValueError(\"plot_datas and dataset_names must have the same length.\")\n",
    "\n",
    "    cmap = plt.cm.plasma\n",
    "    cmap = truncate_cmap(cmap)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        do_show = True\n",
    "    else:\n",
    "        do_show = False\n",
    "    \n",
    "    min_y = np.inf\n",
    "    max_x = -np.inf\n",
    "    for i, (plot_data, dataset_name) in enumerate(zip(plot_datas, dataset_names)):\n",
    "        dataset_name = dataset_names_mapping[dataset_name]\n",
    "        color = cmap(i / (len(dataset_names) - 1))\n",
    "        \n",
    "        # Extract x (size) and y (entCE) from the dicts\n",
    "        x = np.array([point['size'] for point in plot_data])\n",
    "        y = np.array([point['entCE'] for point in plot_data])\n",
    "\n",
    "        min_y = np.minimum(min_y, np.min(y))\n",
    "        max_x = np.maximum(max_x, np.max(x))\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(x, y, color=color)#, label=f\"{dataset_name} (data)\")\n",
    "        \n",
    "        # Best fit line\n",
    "        coeffs = np.polyfit(x, y, 1)  # Linear fit (degree 1)\n",
    "        slope, intercept = coeffs\n",
    "        best_fit_line = slope * x + intercept\n",
    "        ax.plot(x, best_fit_line, color=color, label=\"{}: $Y={:.2f}X^{{{:.2f}}}$\".format(dataset_name, np.exp(intercept), slope))\n",
    "\n",
    "    model_family_to_offset = {\n",
    "        'Llama-3': -0.2,\n",
    "        'Qwen2.5': -0.6,\n",
    "        'pythia': -0.,\n",
    "        'Llama-2': -0.15\n",
    "    }\n",
    "    offset = model_family_to_offset[model_family]\n",
    "    ax.scatter([max_x], [min_y + offset,], alpha=0.) # hacky way to make space for legend\n",
    "    \n",
    "    # Labels and legend\n",
    "    #ax.set_xlabel(\"log(Size)\", fontsize=14, weight='bold')\n",
    "    if show_ylabel:\n",
    "        ax.set_ylabel(\"log(EntCE)\", fontsize=14, weight='bold')\n",
    "    ax.set_ylim(bottom=-3, top=0.3)\n",
    "    ax.set_title(model_family_mapping[model_family], fontsize=18)\n",
    "    ax.legend(loc='lower left')\n",
    "\n",
    "    if do_show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_ent_vs_size_multiple(all_plot_datas, dataset_names, model_families, save_path):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "    for i, (ax, plot_datas, model_family) in enumerate(zip(axs, all_plot_datas, model_families)):\n",
    "        plot_ent_vs_size(plot_datas, dataset_names, model_family, ax=ax, show_ylabel=(i==0))\n",
    "    fig.text(0.5, 0.02, 'log(Model Size)', ha='center', fontsize=14, weight='bold')\n",
    "    plt.tight_layout(rect=[0,0.04,1,1]) \n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71904bb7-17d2-4194-8279-907deac15277",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_datas = []\n",
    "for model_family in model_families:\n",
    "    plot_datas = []\n",
    "    for dataset_name in dataset_names:\n",
    "        plot_data = load_ent_vs_size(save_folder, model_family, dataset_name, max_len)\n",
    "        plot_datas.append(plot_data)\n",
    "    all_plot_datas.append(plot_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976ff16-e054-45a0-8179-c10bcb309a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}/ent_vs_size_plot.png'.format(save_folder)\n",
    "plot_ent_vs_size_multiple([all_plot_datas[3], all_plot_datas[0], all_plot_datas[2], all_plot_datas[1]], \n",
    "                          dataset_names, \n",
    "                          [model_families[3], model_families[0], model_families[2], model_families[1]], \n",
    "                          save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6fa29-37e6-4946-9c05-2e704f3a0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entCE_vs_logloss(folder, model_family, dataset_name, max_len):\n",
    "    suffix = '_logprobs_and_ents.pkl'\n",
    "    all_logprobs_and_ents = load_files(folder, model_family, dataset_name, False, suffix)\n",
    "    all_logprobs_and_ents_inst = load_files(folder, model_family, dataset_name, True, suffix)\n",
    "    plot_data = []\n",
    "    for model_revision_name in all_logprobs_and_ents.keys():\n",
    "        model_name = model_revision_name[:model_revision_name.index('_')]\n",
    "\n",
    "        pts = []\n",
    "\n",
    "        ent_dict = all_logprobs_and_ents[model_revision_name]\n",
    "        all_temps = np.sort(np.array(list(ent_dict.keys())))\n",
    "        for temp in all_temps:\n",
    "            response_logprobs, generation_ents = ent_dict[temp]\n",
    "            response_logprobs = [lp[:max_len] for lp in response_logprobs]\n",
    "            generation_ents = [ents[:max_len] for ents in generation_ents]\n",
    "\n",
    "            ents_over_time, ents_avg, ents_std = compute_time_avg(generation_ents)\n",
    "            logprobs_over_time, logprobs_avg, logprobs_std = compute_time_avg(response_logprobs)\n",
    "\n",
    "            data = {}\n",
    "            data['ent_ce'] = ents_avg - (-1) * logprobs_avg\n",
    "            data['log_loss'] = (-1) * logprobs_avg\n",
    "            data['name'] = '$\\\\tau = {}$'.format(temp)\n",
    "            pts.append(data)\n",
    "\n",
    "        data = {}\n",
    "        data['temp_pts'] = pts\n",
    "        data['name'] = model_name\n",
    "\n",
    "        if '-hf' in model_name: # so that checking 'model_name in model_revision_name_inst' works\n",
    "            model_name = model_name[:model_name.index('-hf')]\n",
    "        for model_revision_name_inst in all_logprobs_and_ents_inst.keys():\n",
    "            if model_name in model_revision_name_inst and 1.0 in all_logprobs_and_ents[model_revision_name].keys():\n",
    "                response_logprobs, generation_ents = all_logprobs_and_ents_inst[model_revision_name_inst][1.0]\n",
    "                response_logprobs = [lp[:max_len] for lp in response_logprobs]\n",
    "                generation_ents = [ents[:max_len] for ents in generation_ents]\n",
    "\n",
    "                ents_over_time, ents_avg, ents_std = compute_time_avg(generation_ents)\n",
    "                logprobs_over_time, logprobs_avg, logprobs_std = compute_time_avg(response_logprobs)\n",
    "                \n",
    "                inst_pt = {}\n",
    "                inst_pt['ent_ce'] = ents_avg - (-1) * logprobs_avg\n",
    "                inst_pt['log_loss'] = (-1) * logprobs_avg\n",
    "                inst_pt['name'] = 'instruct'\n",
    "        \n",
    "        data['inst_pt'] = inst_pt\n",
    "\n",
    "        plot_data.append(data)\n",
    "    return plot_data\n",
    "\n",
    "def plot_entCE_vs_logloss(data, dataset_name, ax=None, fontsizes=(14,18), show_title=True, show_xlabel=True, show_ylabel=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots points from 'temp_pts' with a solid line and labels each point with its 'name'.\n",
    "    Draws a dotted line from the first point in 'temp_pts' to 'inst_pt', and labels 'inst_pt'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (dict): Dictionary containing:\n",
    "                     - 'temp_pts': List of dicts with keys 'log_loss' (x), 'ent_ce' (y), 'name' (label).\n",
    "                     - 'inst_pt': A single dict with the same keys.\n",
    "    \"\"\"\n",
    "    offset_dict = {\n",
    "        'wikitext': 0.04,\n",
    "        'writingprompts': 0.04,\n",
    "        'codecontests': 0.01\n",
    "    }\n",
    "    offset = offset_dict[dataset_name]\n",
    "\n",
    "    \n",
    "    temp_pts = data[\"temp_pts\"]\n",
    "    inst_pt = data[\"inst_pt\"]\n",
    "\n",
    "    cmap = plt.cm.plasma\n",
    "    colors = [cmap(0), cmap(0.5)]\n",
    "    \n",
    "    # Extract x and y coordinates for temp_pts\n",
    "    x_vals = [pt[\"log_loss\"] for pt in temp_pts]\n",
    "    y_vals = [pt[\"ent_ce\"] for pt in temp_pts]\n",
    "    labels = [pt[\"name\"] for pt in temp_pts]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        do_show = True\n",
    "    else:\n",
    "        do_show = False\n",
    "    \n",
    "    # Plot temp_pts as a solid line with markers\n",
    "    ax.plot(x_vals, y_vals, linestyle='-', marker='o', zorder=0.5, color=colors[0])\n",
    "    \n",
    "    # Annotate each point with its name\n",
    "    for pt in temp_pts:\n",
    "        ax.text(pt[\"log_loss\"], pt[\"ent_ce\"] + offset, pt[\"name\"], fontsize=fontsizes[0], ha='center', va='bottom', zorder=1)\n",
    "    \n",
    "    # Plot inst_pt as a single point\n",
    "    ax.scatter(inst_pt[\"log_loss\"], inst_pt[\"ent_ce\"], color=colors[1], marker='D')\n",
    "    \n",
    "    # Annotate inst_pt\n",
    "    ax.text(inst_pt[\"log_loss\"], inst_pt[\"ent_ce\"] + offset, inst_pt[\"name\"], fontsize=fontsizes[0], ha='center', va='bottom', zorder=1)\n",
    "    \n",
    "    # Draw a dotted line from the last temp_pt to inst_pt\n",
    "    last_pt = temp_pts[-1]\n",
    "    ax.plot(\n",
    "        [last_pt[\"log_loss\"], inst_pt[\"log_loss\"]],\n",
    "        [last_pt[\"ent_ce\"], inst_pt[\"ent_ce\"]],\n",
    "        linestyle=':', color=colors[1], zorder=0,\n",
    "    )\n",
    "\n",
    "    # Hacky way to create space for annotation\n",
    "    min_x, max_x = np.min(x_vals + [inst_pt[\"log_loss\"]]), np.max(x_vals + [inst_pt[\"log_loss\"]])\n",
    "    min_y, max_y = np.min(y_vals + [inst_pt[\"ent_ce\"]]), np.max(y_vals + [inst_pt[\"ent_ce\"]])\n",
    "    pad_dict = {\n",
    "        'wikitext': (0.005, 0.15),\n",
    "        'writingprompts': (0.005, 0.17),\n",
    "        'codecontests': (0.003, 0.04)\n",
    "    }\n",
    "    x_pad, y_pad = pad_dict[dataset_name]\n",
    "    ax.plot(\n",
    "        [min_x - x_pad, max_x + x_pad],\n",
    "        [max_y + y_pad, min_y], alpha=0.\n",
    "    )\n",
    "\n",
    "    # draw dashed line at ent_ce = 0\n",
    "    ax.axhline(y=0, linestyle='--', color='gray', zorder=0)\n",
    "    \n",
    "    # Labels and title\n",
    "    if show_xlabel:\n",
    "        ax.set_xlabel(\"Log loss\", fontsize=fontsizes[0], weight='bold')\n",
    "    if show_ylabel:\n",
    "        ax.set_ylabel(\"Entropy calibration error\", fontsize=fontsizes[0], weight='bold')\n",
    "    if show_title:\n",
    "        if title is None:\n",
    "            title = dataset_names_mapping[dataset_name]\n",
    "        ax.set_title(title, fontsize=fontsizes[1])\n",
    "    \n",
    "    # Legend\n",
    "    #ax.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    if do_show:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_entCE_vs_logloss_partial(plot_datas, model_name, dataset_names, save_path):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    for j, (ax, plot_data, dataset_name) in enumerate(zip(axs, plot_datas, dataset_names)):\n",
    "        for data in plot_data:\n",
    "            if data['name'] == model_name:\n",
    "                plot_entCE_vs_logloss(data, dataset_name, ax=ax, fontsizes=(14,18), show_title=True, show_xlabel=(j==1), show_ylabel=(j==0))\n",
    "    plt.tight_layout(rect=[0,0,1,0.9])\n",
    "    fig.text(0.51, 0.92, model_name, ha='center', fontsize=18, weight='bold')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "def plot_entCE_vs_logloss_multiple(plot_datas, dataset_names, save_path):\n",
    "    model_names = [(data['name'], get_model_size(data['name'] + '_revision=main')) for data in plot_datas[0]]\n",
    "    model_names.sort(key=lambda x: x[1])\n",
    "    model_names = [model_name for model_name, size in model_names]\n",
    "    #print(model_names)\n",
    "    \n",
    "    fig, all_axs = plt.subplots(len(model_names), 3, figsize=(16, len(model_names) * 4))\n",
    "    for i, (axs, model_name) in enumerate(zip(all_axs, model_names)):\n",
    "        for j, (ax, plot_data, dataset_name) in enumerate(zip(axs, plot_datas, dataset_names)):\n",
    "            for data in plot_data:\n",
    "                if data['name'] == model_name:\n",
    "                    if i == 0:\n",
    "                        if j == 1:\n",
    "                            title = \"{}\\n{}\".format(dataset_name, model_name)\n",
    "                        else:\n",
    "                            title = \"{}\\n\".format(dataset_name)\n",
    "                    else:\n",
    "                        title = model_name\n",
    "                    plot_entCE_vs_logloss(\n",
    "                        data, dataset_name, ax=ax, fontsizes=(14,18), title=title,\n",
    "                        show_title=(i==0 or j==1), show_xlabel=(i==len(model_names)-1 and j==1), show_ylabel=False\n",
    "                    )\n",
    "    #plt.tight_layout()\n",
    "    #plt.tight_layout(rect=[0,0,1,0.9])\n",
    "    #fig.text(0.51, 0.92, model_name, ha='center', fontsize=18, weight='bold')\n",
    "    fig.text(0.015, 0.5, 'Entropy calibration error', ha='center', fontsize=14, weight='bold', rotation='vertical')\n",
    "    plt.tight_layout(rect=[0.02,0,1,1])\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf60c9-6d7a-450b-a31b-9290b99cc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_families = ('Llama-2', 'Llama-3', 'Qwen2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ae4cb-20cd-4c2f-9e29-1b8bd2cdaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plot_datas = []\n",
    "for model_family in model_families:\n",
    "    plot_datas = []\n",
    "    for dataset_name in dataset_names:\n",
    "        plot_data = load_entCE_vs_logloss(save_folder, model_family, dataset_name, max_len)\n",
    "        plot_datas.append(plot_data)\n",
    "    all_plot_datas.append(plot_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075325c-4af3-4280-abb0-617357447cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '{}/entCE_vs_logloss_plot_partial.png'.format(save_folder)\n",
    "plot_entCE_vs_logloss_partial(all_plot_datas[2], 'Qwen2.5-14B', dataset_names, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57595e-baee-49a1-a4b1-911d3e6245b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "save_path = '{}/entCE_vs_logloss_plot_qwen.png'.format(save_folder)\n",
    "plot_entCE_vs_logloss_multiple(all_plot_datas[2], dataset_names, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d10e2e-314a-43fb-98ba-f0d0ff08d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "save_path = '{}/entCE_vs_logloss_plot_llama_2.png'.format(save_folder)\n",
    "plot_entCE_vs_logloss_multiple(all_plot_datas[0], dataset_names, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21887141-b714-4829-af51-49097de13fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "save_path = '{}/entCE_vs_logloss_plot_llama_3.png'.format(save_folder)\n",
    "plot_entCE_vs_logloss_multiple(all_plot_datas[1], dataset_names, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
